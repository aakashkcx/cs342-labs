{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS342 Machine Learning\n",
    "# Lab 4: Linear regression\n",
    "\n",
    "## Department of Computer Science, University of Warwick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab focuses on the use of regularization for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data files for the lab\n",
    "\n",
    "If working on one of the DCS machines, the data may be found here:\n",
    "\n",
    "```/modules/cs342/2020/lab4/data/prostate_data.csv ```\n",
    "\n",
    "You may load the data directly from that directory.\n",
    "\n",
    "If you are using your own machine, copy the data across by running the following command in a terminal window using the remote node corresponding to your username. The name of this remote node uses the last two digits of your username in the form remote-nn, for example, if your username is u1234567 you would connect to remote-67.dcs.warwick.ac.uk (recall to use your USERNAME and correpsonding REMOTE_NN):\n",
    "\n",
    "```scp USERNAME@REMOTE_NN.dcs.warwick.ac.uk:/modules/cs342/2020/lab4/data/prostate_data.csv .```\n",
    "\n",
    "After entering your DCS password, this will copy the data to your current working directory. You should now have the following file:\n",
    "```\n",
    "├──[your working directory]\n",
    "   └── prostate_data.csv\n",
    "```\n",
    "**Please make sure to use the correct path to these files when working on your own machine. The scripts below assume you are working on the DCS machines. Recall that the *.ipynb file (this file) should be in your working directory.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prostate dataset (file *prostate_data.csv* see: https://web.stanford.edu/~hastie/ElemStatLearn//datasets/prostate.data) will be used to predict the numerical target variable *lpsa* based on 8 features (*lcavol, lweight, age, lbph, svi, lcp, gleason, pgg45*). There are 97 observations in total. The last column is a Train/Predict flag to be used to separate the observations into two subsets. The *train = T* subset will be used for model fitting and cross-validation (CV), while the *train = F* subset will be used for testing after model selection and training.\n",
    "\n",
    "Import the data into a Pandas data frame and standardize the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      lcavol   lweight       age      lbph       svi       lcp   gleason  \\\n",
      "0  -1.637356 -2.006212 -1.862426 -1.024706 -0.522941 -0.863171 -1.042157   \n",
      "1  -1.988980 -0.722009 -0.787896 -1.024706 -0.522941 -0.863171 -1.042157   \n",
      "2  -1.578819 -2.188784  1.361163 -1.024706 -0.522941 -0.863171  0.342627   \n",
      "3  -2.166917 -0.807994 -0.787896 -1.024706 -0.522941 -0.863171 -1.042157   \n",
      "4  -0.507874 -0.458834 -0.250631 -1.024706 -0.522941 -0.863171 -1.042157   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "92  1.255920  0.577607  0.555266 -1.024706  1.892548  1.073572  0.342627   \n",
      "93  2.096506  0.625489 -2.668323 -1.024706  1.892548  1.679542  0.342627   \n",
      "94  1.321402 -0.543304 -1.593794 -1.024706  1.892548  1.890377  0.342627   \n",
      "95  1.300290  0.338384  0.555266  1.004813  1.892548  1.242632  0.342627   \n",
      "96  1.800367  0.807764  0.555266  0.232904  1.892548  2.205279  0.342627   \n",
      "\n",
      "       pgg45  \n",
      "0  -0.864467  \n",
      "1  -0.864467  \n",
      "2  -0.155348  \n",
      "3  -0.864467  \n",
      "4  -0.864467  \n",
      "..       ...  \n",
      "92  1.262889  \n",
      "93  0.553770  \n",
      "94 -0.509907  \n",
      "95  1.972007  \n",
      "96 -0.155348  \n",
      "\n",
      "[97 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import prostate dataset\n",
    "prostate = pd.read_csv(\"./prostate_data.csv\")\n",
    "\n",
    "#standardise features\n",
    "features = prostate.drop([\"lpsa\", \"train\"], axis=1)\n",
    "standardised = (features - features.mean()) / features.std()\n",
    "\n",
    "print(standardised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-regularized linear regression \n",
    "\n",
    "Scikit-learn has a plethora of linear models:http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model. It includes all the implementations needed for this lab, including implementations that employ cross-validation to select hyper-parameters for regularization. \n",
    "\n",
    "Fit a non-regularized linear regression model to the *train = T* subset. Use your fitted model to predict the target variable in the *train = F* subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.96903844, 1.16995577, 1.26117929, 1.88375914, 2.54431886,\n",
       "       1.93275402, 2.04233571, 1.83091625, 1.99115929, 1.32347076,\n",
       "       2.93843111, 2.20314404, 2.166421  , 2.79456237, 2.67466879,\n",
       "       2.18057291, 2.40211068, 3.02351576, 3.21122283, 1.38441459,\n",
       "       3.41751878, 3.70741749, 2.54118337, 2.72969658, 2.64055575,\n",
       "       3.48060024, 3.17136269, 3.2923494 , 3.11889686, 3.76383999])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Fit and test a non-regularized linear regression model\n",
    "\n",
    "X = standardised[prostate[\"train\"] == \"T\"]\n",
    "y = prostate[prostate[\"train\"] == \"T\"][\"lpsa\"]\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "X_test = standardised[prostate[\"train\"] == \"F\"]\n",
    "y_test = prostate[prostate[\"train\"] == \"F\"][\"lpsa\"]\n",
    "\n",
    "reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-regularized linear regression (ridge regression)\n",
    "\n",
    "Fit an L2-regularized linear regression model to the *train = T* subset once you use 3-fold CV  to tune the hyper-parameter for regularization. Use the model fitted with the best hyper-parameter to predict the target variable in the *train = F* subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0: 6.99484229135647\n",
      "alpha = 1: 6.910620619160588\n",
      "alpha = 2: 6.847810511130106\n",
      "alpha = 3: 6.799303327633505\n",
      "alpha = 4: 6.760904753682314\n",
      "alpha = 5: 6.729940094625884\n",
      "alpha = 6: 6.704607553167296\n",
      "alpha = 7: 6.6836425963121835\n",
      "alpha = 8: 6.666128958415913\n",
      "alpha = 9: 6.651385437101944\n",
      "alpha = 10: 6.638894725825529\n",
      "alpha = 11: 6.628256894306021\n",
      "alpha = 12: 6.619157986566301\n",
      "alpha = 13: 6.611348243002422\n",
      "alpha = 14: 6.604626644299121\n",
      "alpha = 15: 6.5988297207843045\n",
      "alpha = 16: 6.593823307234456\n",
      "alpha = 17: 6.589496373309037\n",
      "alpha = 18: 6.585756343106307\n",
      "alpha = 19: 6.582525500251362\n",
      "alpha = 20: 6.57973819572553\n",
      "alpha = 21: 6.5773386570446135\n",
      "alpha = 22: 6.575279253243868\n",
      "alpha = 23: 6.573519109079748\n",
      "alpha = 24: 6.572022989431187\n",
      "alpha = 25: 6.570760394667133\n",
      "alpha = 26: 6.56970482211949\n",
      "alpha = 27: 6.568833159362072\n",
      "alpha = 28: 6.568125182839343\n",
      "alpha = 29: 6.567563141270651\n",
      "alpha = 30: 6.5671314077072465\n",
      "alpha = 31: 6.5668161875169515\n",
      "alpha = 32: 6.566605272185548\n",
      "alpha = 33: 6.566487830850032\n",
      "alpha = 34: 6.566454233060598\n",
      "alpha = 35: 6.566495897510947\n",
      "alpha = 36: 6.566605162459129\n",
      "alpha = 37: 6.566775174342628\n",
      "alpha = 38: 6.566999791716531\n",
      "alpha = 39: 6.567273502146069\n",
      "alpha = 40: 6.567591350091145\n",
      "alpha = 41: 6.567948874150081\n",
      "alpha = 42: 6.568342052298985\n",
      "alpha = 43: 6.568767253983442\n",
      "alpha = 44: 6.5692211981006485\n",
      "alpha = 45: 6.569700916059841\n",
      "alpha = 46: 6.5702037192331\n",
      "alpha = 47: 6.570727170211941\n",
      "alpha = 48: 6.571269057371477\n",
      "alpha = 49: 6.571827372316193\n",
      "Best Error: 6.566454233060598\n",
      "Best Hyper-Parameter: 34\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Fit and test an L2-regularized linear regression model\n",
    "\n",
    "\n",
    "def error(predicted: np.ndarray, true: np.ndarray) -> float:\n",
    "    errors = (predicted - true) ** 2\n",
    "    return math.sqrt(sum(errors))\n",
    "\n",
    "\n",
    "def ridge(alpha: float, X: pd.DataFrame, y: pd.DataFrame, test: pd.DataFrame):\n",
    "    clf = Ridge(alpha=alpha)\n",
    "    clf.fit(X, y)\n",
    "    return clf.predict(test)\n",
    "\n",
    "\n",
    "def threefold(alpha: float, X: pd.DataFrame, y: pd.DataFrame):\n",
    "    kf = KFold(n_splits=3, random_state=None)\n",
    "    errors = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        prediction = ridge(alpha, X_train, y_train, X_test)\n",
    "        errors.append(error(prediction, y_test))\n",
    "    return sum(errors) / len(errors)\n",
    "\n",
    "\n",
    "minimum = np.inf\n",
    "param = None\n",
    "for alpha in range(50):\n",
    "    value = threefold(alpha, X, y)\n",
    "    print(f\"alpha = {alpha}:\", value)\n",
    "    if value < minimum:\n",
    "        minimum = value\n",
    "        param = alpha\n",
    "\n",
    "print(\"Best Error:\", minimum)\n",
    "print(\"Best Hyper-Parameter:\", param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1-regularized linear regression (lasso regression)\n",
    "\n",
    "Fit an L1-regularized linear regression model to the *train = T* subset once you use 3-fold CV  to tune the hyper-parameter for regularization. Use the model fitted with the best hyper-parameter to predict the target variable in the *train = F* subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and test an L1-regularized linear regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which model performes the best for this dataset?\n",
    "2. Which features are irrelevant for this task? **Hint:** display the learned coefficients (weights) for each model and recall which type of regularization allows for feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
